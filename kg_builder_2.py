import pandas as pd
import numpy as np
from neo4j import GraphDatabase
import networkx as nx
from node2vec import Node2Vec
from random import shuffle
import traceback
import sys
from random import randint
import os


def connect_to_neo4j(uri="neo4j+s://1f5f8a14.databases.neo4j.io", user="neo4j", password="3xhy4XKQSsSLIT7NI-w9m4Z7Y_WcVnL1hDQkWTMIoMQ"):
    print(f"ğŸŒ Connecting to Neo4j Aura: {uri}", flush=True)
    return GraphDatabase.driver(uri, auth=(user, password))

def parse_csv(file_path):
    df = pd.read_csv(file_path, sep=";", encoding="utf-8-sig")
    df.columns = [col.strip() for col in df.columns]
    df = df.apply(lambda col: col.str.strip() if col.dtypes == 'object' else col)

    numeric_cols = ['Case_No', 'A1', 'A2', 'A3', 'A4', 'A5', 'A6', 'A7', 'A8', 'A9', 'A10', 'Age_Mons', 'Qchat-10-Score']
    for col in numeric_cols:
        df[col] = pd.to_numeric(df[col].astype(str).str.replace(",", "."), errors='coerce')

    print("âœ… ÎšÎ±Î¸Î±ÏÎ¯ÏƒÏ„Î·ÎºÎ±Î½ Î¿Î¹ ÏƒÏ„Î®Î»ÎµÏ‚:", df.columns.tolist(), flush=True)
    return df.dropna()

def create_nodes(tx, df):
    for q in [f"A{i}" for i in range(1, 11)]:
        tx.run("MERGE (:BehaviorQuestion {name: $q})", q=q)

    for column in ["Sex", "Ethnicity", "Jaundice", "Family_mem_with_ASD"]:
        for val in df[column].dropna().unique():
            tx.run("MERGE (:DemographicAttribute {type: $type, value: $val})", type=column, val=val)

    for val in df["Who_completed_the_test"].dropna().unique():
        tx.run("MERGE (:SubmitterType {type: $val})", val=val)

def create_relationships(tx, df):
    case_data = []
    answer_data, demo_data, submitter_data = [], [], []

    for _, row in df.iterrows():
        case_id = int(row["Case_No"])
        upload_id = str(case_id)
        case_data.append({"id": case_id, "upload_id": upload_id})

        for q in [f"A{i}" for i in range(1, 11)]:
            answer_data.append({"upload_id": upload_id, "q": q, "val": int(row[q])})

        for col in ["Sex", "Ethnicity", "Jaundice", "Family_mem_with_ASD"]:
            demo_data.append({"upload_id": upload_id, "type": col, "val": row[col]})

        submitter_data.append({"upload_id": upload_id, "val": row["Who_completed_the_test"]})

    tx.run("UNWIND $data as row MERGE (c:Case {id: row.id}) SET c.upload_id = row.upload_id, c.embedding = null", data=case_data)
    tx.run("""
        UNWIND $data as row
        MATCH (q:BehaviorQuestion {name: row.q})
        MATCH (c:Case {upload_id: row.upload_id})
        MERGE (c)-[:HAS_ANSWER {value: row.val}]->(q)
    """, data=answer_data)
    tx.run("""
        UNWIND $data as row
        MATCH (d:DemographicAttribute {type: row.type, value: row.val})
        MATCH (c:Case {upload_id: row.upload_id})
        MERGE (c)-[:HAS_DEMOGRAPHIC]->(d)
    """, data=demo_data)
    tx.run("""
        UNWIND $data as row
        MATCH (s:SubmitterType {type: row.val})
        MATCH (c:Case {upload_id: row.upload_id})
        MERGE (c)-[:SUBMITTED_BY]->(s)
    """, data=submitter_data)
def create_similarity_relationships(tx, df, max_pairs=10000):
    pairs = set()
    
    # 1. Î£Ï…Î¼Ï€ÎµÏÎ¹Ï†Î¿ÏÎ¹ÎºÎ® Î¿Î¼Î¿Î¹ÏŒÏ„Î·Ï„Î± (A1-A10)
    for i, row1 in df.iterrows():
        for j, row2 in df.iloc[i+1:].iterrows():
            if sum(row1[f'A{k}'] == row2[f'A{k}'] for k in range(1,11)) >= 7:
                pairs.add((int(row1['Case_No']), int(row2['Case_No'])))

    # 2. Î”Î·Î¼Î¿Î³ÏÎ±Ï†Î¹ÎºÎ® Î¿Î¼Î¿Î¹ÏŒÏ„Î·Ï„Î± (Î´Î¹Î¿ÏÎ¸Ï‰Î¼Î­Î½Î¿ groupby)
    demo_cols = ['Sex', 'Ethnicity', 'Jaundice', 'Family_mem_with_ASD']
    for col in demo_cols:
        # Î£Ï‰ÏƒÏ„Î® Ï‡ÏÎ®ÏƒÎ· groupby Î¼Îµ Î­Î½Î± Î¼ÏŒÎ½Î¿ ÎºÏÎ¹Ï„Î®ÏÎ¹Î¿
        grouped = df.groupby(col)['Case_No'].apply(list)
        for case_list in grouped:
            for i in range(len(case_list)):
                for j in range(i+1, len(case_list)):
                    pairs.add((int(case_list[i]), int(case_list[j])))

    # Î•Ï†Î±ÏÎ¼Î¿Î³Î® Î¿ÏÎ¯Î¿Ï… ÎºÎ±Î¹ Ï„Ï…Ï‡Î±Î¹Î¿Ï€Î¿Î¯Î·ÏƒÎ·
    pair_list = list(pairs)[:max_pairs]
    shuffle(pair_list)

    # Î•Î¹ÏƒÎ±Î³Ï‰Î³Î® ÏƒÏ‡Î­ÏƒÎµÏ‰Î½ ÏƒÏ„Î· Neo4j
    tx.run("""
        UNWIND $batch AS pair
        MATCH (c1:Case {id: pair.id1}), (c2:Case {id: pair.id2})
        MERGE (c1)-[:SIMILAR_TO]->(c2)
    """, batch=[{'id1':x, 'id2':y} for x,y in pair_list])

def generate_embeddings(driver):
    temp_folder_path = os.path.join(os.getcwd(), 'node2vec_temp')
    os.makedirs(temp_folder_path, exist_ok=True)
    G = nx.Graph()
    
    # Î•Ï€Î¹Ï„Î¬Ï‡Ï…Î½ÏƒÎ· Ï†ÏŒÏÏ„Ï‰ÏƒÎ·Ï‚ Î³ÏÎ±Ï†Î®Î¼Î±Ï„Î¿Ï‚ Î¼Îµ single query
    with driver.session() as session:
        result = session.run("""
            MATCH (c:Case)
            OPTIONAL MATCH (c)-[r:HAS_ANSWER|HAS_DEMOGRAPHIC|SUBMITTED_BY|GRAPH_SIMILARITY]->(other)
            WHERE c.id IS NOT NULL AND other.id IS NOT NULL
            RETURN c.id AS node_id, collect(DISTINCT other.id) AS neighbors
        """)
        
        for record in result:
            node_id = str(record["node_id"])
            G.add_node(node_id)
            for neighbor in record["neighbors"]:
                G.add_edge(node_id, str(neighbor))

    # ÎˆÎ»ÎµÎ³Ï‡Î¿Ï‚ Î³ÏÎ±Ï†Î®Î¼Î±Ï„Î¿Ï‚
    if len(G.nodes) < 10:  # Î‘ÏÎ¾Î·ÏƒÎ· ÎµÎ»Î¬Ï‡Î¹ÏƒÏ„Î¿Ï… Î¿ÏÎ¯Î¿Ï… Î³Î¹Î± ÎºÎ±Î»ÏÏ„ÎµÏÎ± embeddings
        raise ValueError(f"âŒ Not enough nodes ({len(G.nodes)}) for meaningful embeddings")

    print(f"ğŸ“Š Graph stats: {len(G.nodes)} nodes, {len(G.edges)} edges")
    
    # Î’ÎµÎ»Ï„Î¹ÏƒÏ„Î¿Ï€Î¿Î¹Î·Î¼Î­Î½Î¿Î¹ Ï€Î±ÏÎ¬Î¼ÎµÏ„ÏÎ¿Î¹ Node2Vec
    node2vec = Node2Vec(
        G,
        dimensions=128,       # Î£Ï…Î½Î­Ï€ÎµÎ¹Î± Î¼Îµ Ï„Î¿ generate_case_embedding.py
        walk_length=30,       # Î‘Ï…Î¾Î·Î¼Î­Î½Î¿ Î³Î¹Î± ÎºÎ±Î»ÏÏ„ÎµÏÎ· ÎµÎ¾ÎµÏÎµÏÎ½Î·ÏƒÎ· Î³ÏÎ±Ï†Î®Î¼Î±Ï„Î¿Ï‚
        num_walks=200,        # Î ÎµÏÎ¹ÏƒÏƒÏŒÏ„ÎµÏÎ± walks Î³Î¹Î± ÏƒÏ„Î±Î¸ÎµÏÏŒÏ„ÎµÏÎ± Î±Ï€Î¿Ï„ÎµÎ»Î­ÏƒÎ¼Î±Ï„Î±
        workers=4,            # Î‘Î¾Î¹Î¿Ï€Î¿Î¯Î·ÏƒÎ· Ï€Î¿Î»Ï…Ï€ÏÏÎ·Î½Ï‰Î½ ÎµÏ€ÎµÎ¾ÎµÏÎ³Î±ÏƒÏ„ÏÎ½
        p=1.0,                # Î Î±ÏÎ¬Î¼ÎµÏ„ÏÎ¿Ï‚ p Î³Î¹Î± BFS
        q=0.5,                # Î Î±ÏÎ¬Î¼ÎµÏ„ÏÎ¿Ï‚ q Î³Î¹Î± DFS
        temp_folder=os.path.join(os.getcwd(), 'node2vec_temp')  # Î ÏÎ¿ÏƒÏ‰ÏÎ¹Î½ÏŒÏ‚ Ï†Î¬ÎºÎµÎ»Î¿Ï‚
    )

    try:
        model = node2vec.fit(
            window=10,
            min_count=1,
            batch_words=128
        )
        
        # Î•Ï€Î¹Ï„Î¬Ï‡Ï…Î½ÏƒÎ· Î±Ï€Î¿Î¸Î®ÎºÎµÏ…ÏƒÎ·Ï‚ Î¼Îµ batch query
        with driver.session() as session:
            batch = []
            for node_id in G.nodes():
                embedding = model.wv[str(node_id)].tolist()
                batch.append({"node_id": int(node_id), "embedding": embedding})
                
                if len(batch) >= 1000:  # Batch ÎµÎ½Î·Î¼ÎµÏÏÏƒÎµÏ‰Î½
                    session.run("""
                        UNWIND $batch AS item
                        MATCH (c:Case {id: item.node_id})
                        SET c.embedding = item.embedding
                    """, {"batch": batch})
                    batch = []
            
            if batch:  # Î¤ÎµÎ»ÎµÏ…Ï„Î±Î¯Î¿ batch
                session.run("""
                    UNWIND $batch AS item
                    MATCH (c:Case {id: item.node_id})
                    SET c.embedding = item.embedding
                """, {"batch": batch})

        print(f"âœ… Successfully saved embeddings for {len(G.nodes)} nodes")
        return True

    except Exception as e:
        print(f"âŒ Failed to generate embeddings: {str(e)}")
        return False

def build_graph():
    driver = connect_to_neo4j()
    file_path = "https://raw.githubusercontent.com/GiorgosBouh/llm2cypher-asd-app/main/Toddler_Autism_dataset_July_2018_2.csv"

    try:
        df = parse_csv(file_path)
        print("ğŸ§  First row:", df.iloc[0].to_dict(), flush=True)

        with driver.session() as session:
            print("ğŸ§¹ Î”Î¹Î±Î³ÏÎ±Ï†Î® ÏŒÎ»Ï‰Î½ Ï„Ï‰Î½ ÎºÏŒÎ¼Î²Ï‰Î½ ÎºÎ±Î¹ ÏƒÏ‡Î­ÏƒÎµÏ‰Î½...", flush=True)
            session.run("MATCH (n) DETACH DELETE n")

            print("â³ Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± ÎºÏŒÎ¼Î²Ï‰Î½...", flush=True)
            session.execute_write(create_nodes, df)

            print("â³ Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± ÏƒÏ‡Î­ÏƒÎµÏ‰Î½...", flush=True)
            session.execute_write(create_relationships, df)

            print("â³ Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± ÏƒÏ‡Î­ÏƒÎµÏ‰Î½ Î¿Î¼Î¿Î¹ÏŒÏ„Î·Ï„Î±Ï‚...", flush=True)
            session.execute_write(create_similarity_relationships, df)

        print("â³ Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± embeddings...", flush=True)
        generate_embeddings(driver)

        print("âœ… ÎŸÎ»Î¿ÎºÎ»Î·ÏÏÎ¸Î·ÎºÎµ ÎµÏ€Î¹Ï„Ï…Ï‡ÏÏ‚!", flush=True)
        sys.exit(0)

    except Exception as e:
        print(f"âŒ Î£Ï†Î¬Î»Î¼Î±: {str(e)}", flush=True)
        traceback.print_exc()
        sys.exit(1)

    finally:
        driver.close()

if __name__ == "__main__":
    build_graph()